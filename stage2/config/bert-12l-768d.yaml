data:
  datasets:
  - notebook_dir: ../resources/ai4code/train_cleaned
    embedding_dir: ../resources/embeddings
    orders: ../resources/ai4code/train_orders.csv
    clusters: ../resources/ai4code/train_clusters.csv
  - notebook_dir: ../resources/codeparrot/train_cleaned
    embedding_dir: ../resources/embeddings
    orders: ../resources/codeparrot/train_orders.csv 
  - notebook_dir: ../resources/kgtorrent/train_cleaned
    embedding_dir: ../resources/embeddings
    orders: ../resources/kgtorrent/train_orders.csv
  validation_ratio: 0.1
  max_length: 512
  num_workers: 0

model:
  vocab_size: 1
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 513
  type_vocab_size: 1
  layer_norm_eps: 1e-5
  num_labels: 1

optim:
  optimizer:
    lr: 1e-4
    betas: [0.9, 0.98]
    eps: 1e-6
    weight_decay: 0.01
  scheduler:
    name: linear
    num_warmup_steps: 10000
    num_training_steps: 1500000

train:
  name: bert-12l-768d
  batch_size: 64
  gradient_clip_val: 0.0
  accumulate_grad_batches: 1
  validation_interval: 10.0
  log_every_n_steps: 50
